{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    this file are mainly used the function from sklearn\n",
    "    including classifier, feature selection, and plot\n",
    "'''\n",
    "\n",
    "import couchdb\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_regression, f_classif\n",
    "from collections import Counter\n",
    "import operator\n",
    "import json\n",
    "import string\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from couchdb.mapping import Document, TextField, IntegerField, DateTimeField, BooleanField\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import configparser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    connect to db and read data from db\n",
    "'''\n",
    "\n",
    "remote_ip = 'http://115.146.92.169:5984'\n",
    "couch_azure = couchdb.Server(remote_ip)\n",
    "remote_train_db = couch_azure['train_data']\n",
    "print(remote_train_db)\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for _id in remote_train_db:\n",
    "    tweet = remote_train_db.get(_id)\n",
    "    texts.append(tweet['text'])\n",
    "    labels.append(tweet['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    store counter and tf-id file\n",
    "'''\n",
    "TFID_FILE = 'tfid_counter_model40000_f_class_cloud.pkl'\n",
    "WRITE = 'wb'\n",
    "COUNTER_FILE = 'counter_model40000_f_class_cloud.pkl'\n",
    "WORD_LEVEL = 'word'\n",
    "\n",
    "X_train_features = []\n",
    "X_train_features_tfid = []\n",
    "down_texts = []\n",
    "down_labels = []\n",
    "\n",
    "\n",
    "# get best 5000 vocabulary from 7000\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5)\n",
    "X_train_features = vectorizer.fit_transform(texts)\n",
    "ch2 = SelectKBest(f_classif, k = 6000)\n",
    "X_train_features = ch2.fit_transform(X_train_features, labels)\n",
    "voc = np.asarray(vectorizer.get_feature_names())[ch2.get_support()]\n",
    "\n",
    "# save the model\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, vocabulary = voc)\n",
    "X_train_features_tfid = vectorizer.fit_transform(texts)\n",
    "pickle.dump(vectorizer.vocabulary_,open(TFID_FILE, WRITE))\n",
    "vectorizer = CountVectorizer(analyzer = WORD_LEVEL, vocabulary = voc)\n",
    "X_train_features = vectorizer.fit_transform(texts)\n",
    "pickle.dump(vectorizer.vocabulary_,open(COUNTER_FILE, WRITE))\n",
    "\n",
    "X_train_features = X_train_features.toarray()\n",
    "\n",
    "X_train_features_tfid = X_train_features_tfid.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    balance the file\n",
    "'''\n",
    "count_pos = 0\n",
    "count_neg = 0\n",
    "count_neu = 0\n",
    "down_texts = []\n",
    "down_labels = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 1 and count_pos < 50000:\n",
    "        down_texts.append(X_train_features[i])\n",
    "        down_labels.append(labels[i])\n",
    "        count_pos += 1\n",
    "    elif labels[i] == -1:\n",
    "        down_texts.append(X_train_features[i])\n",
    "        down_labels.append(labels[i])\n",
    "        count_neg += 1\n",
    "    elif labels[i] == 0 and count_neu < 50000:\n",
    "        down_texts.append(X_train_features[i])\n",
    "        down_labels.append(labels[i])\n",
    "        count_neu += 1\n",
    "print(count_pos)\n",
    "print(count_neg)\n",
    "print(count_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    logistic regression\n",
    "    show cross validation first\n",
    "    then hold-out\n",
    "'''\n",
    "lr = LogisticRegression()\n",
    "scores = cross_val_score(lr, down_texts, down_labels, cv=10)\n",
    "score = 0.0\n",
    "for item in scores:\n",
    "\tscore += item\n",
    "print(score/10)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "\tdown_texts, down_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# logistic regression for cross validation\n",
    "\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "result = lr.predict(X_test)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    save logistic regression\n",
    "'''\n",
    "counter_file = 'counter_f_classi_40000_lr_cloud.pkl'\n",
    "write_byte = 'wb'\n",
    "with open(counter_file, write_byte) as fid:\n",
    "    pickle.dump(lr, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    multinomial naive bayes\n",
    "    show cross validation first\n",
    "    then hold-out\n",
    "'''\n",
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, down_texts, down_labels, cv=10)\n",
    "score = 0.0\n",
    "for item in scores:\n",
    "\tscore += item\n",
    "print(score/10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    save multinomial naive bayes\n",
    "'''\n",
    "counter_file = 'counter_f_class_6000_40000neg_mnb_cloud.pkl'\n",
    "write_byte = 'wb'\n",
    "with open('counter_f_class_6000_40000neg_mnb_cloud.pkl', 'wb') as fid:\n",
    "    pickle.dump(clf, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    neural network\n",
    "    show hold-out\n",
    "'''\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "neural = MLPClassifier(solver='sgd', alpha=1e-5, activation = 'tanh',\n",
    "                     hidden_layer_sizes=(3, 3), random_state=1, verbose = True)\n",
    "\n",
    "neural.fit(X_train, y_train) \n",
    "neural.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    show learning curve with logistic regression\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Logistic regression chi2)\"\n",
    "\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = logistic regression()\n",
    "plot_learning_curve(estimator, title, X_train, y_train, ylim=(0.7, 1.01), cv=10)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    show learning curve with Multinomial Naive Bayes\n",
    "'''\n",
    "\n",
    "title = \"Learning Curves (Multinomial Naive Bayes chi2)\"\n",
    "\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = MultinomialNB()\n",
    "plot_learning_curve(estimator, title, X_train, y_train, ylim=(0.7, 1.01), cv=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    show learning curve with Neural Network\n",
    "'''\n",
    "\n",
    "title = \"Learning Curves (Neural Network chi 2)\"\n",
    "\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "new_estimator = MLPClassifier(solver='sgd', alpha=1e-5, activation = 'tanh',\n",
    "                     hidden_layer_sizes=(3, 3), random_state=1, verbose = True, shuffle=True, early_stopping=True)\n",
    "\n",
    "plot_learning_curve(new_estimator, title, down_texts, down_labels, ylim=(0.7, 1.01), cv=10)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
